{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16621ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 08:25:28.180813: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-09 08:25:28.180840: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier, KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB, CategoricalNB\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize, LabelEncoder, MinMaxScaler\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, det_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense, Activation, GRU\n",
    "from sklearn.preprocessing import normalize, LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a5d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usefull fucntions\n",
    "def remove_nan_values(sample_array:np.array) -> np.array:\n",
    "    for sample_index, i in enumerate(sample_array):\n",
    "        for value_index, j in enumerate(i):\n",
    "            if j==' ':\n",
    "               sample_array[sample_index][value_index] = 0.0\n",
    "            elif type(j) == str:\n",
    "                sample_array[sample_index][value_index] = int(j)\n",
    "            elif np.isnan(j):\n",
    "               sample_array[sample_index][value_index] = 0.0\n",
    "    return sample_array\n",
    "\n",
    "def remove_1d_array_nan_values(sample_array: np.array, attack_categories: dict) -> np.array:\n",
    "    for value_index, j in enumerate(sample_array):\n",
    "        if j==' ':\n",
    "           sample_array[value_index] = 0.0\n",
    "        elif type(j) == str:\n",
    "            try:\n",
    "                sample_array[value_index] = int(j)\n",
    "            except:\n",
    "                sample_array[value_index] = attack_categories[j]\n",
    "        elif np.isnan(j):\n",
    "           sample_array[value_index] = 0.0\n",
    "    return sample_array\n",
    "\n",
    "def save_model(model, name, prefix) -> None:\n",
    "    \"\"\"Function responsible for saving trained model. It must be called\n",
    "    after defining, training and predict.\n",
    "\n",
    "    :param: None\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    filename = f\"{name}_{prefix}.sav\"\n",
    "    pickle.dump(model, open(filename, 'wb'))\n",
    "    \n",
    "def load_model(filepath):\n",
    "    \"\"\"Function responsible for load model.\n",
    "\n",
    "    :param: None\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    return pickle.load(open(filepath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adf96ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkubita/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (7,9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# load data to pandas DataFrame\n",
    "\n",
    "df_path = '/home/mkubita/Pulpit/Praca Magisterska/Zbiory danych/BoT_IoT'\n",
    "\n",
    "csv_files = [os.path.join(df_path, file) for file in os.listdir(df_path)]\n",
    "# load multiple files\n",
    "\n",
    "li = []\n",
    "df = None\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c0f364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pkSeqID</th>\n",
       "      <th>stime</th>\n",
       "      <th>flgs</th>\n",
       "      <th>flgs_number</th>\n",
       "      <th>proto</th>\n",
       "      <th>proto_number</th>\n",
       "      <th>saddr</th>\n",
       "      <th>sport</th>\n",
       "      <th>daddr</th>\n",
       "      <th>dport</th>\n",
       "      <th>...</th>\n",
       "      <th>AR_P_Proto_P_DstIP</th>\n",
       "      <th>N_IN_Conn_P_DstIP</th>\n",
       "      <th>N_IN_Conn_P_SrcIP</th>\n",
       "      <th>AR_P_Proto_P_Sport</th>\n",
       "      <th>AR_P_Proto_P_Dport</th>\n",
       "      <th>Pkts_P_State_P_Protocol_P_DestIP</th>\n",
       "      <th>Pkts_P_State_P_Protocol_P_SrcIP</th>\n",
       "      <th>attack</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000001</td>\n",
       "      <td>1.528096e+09</td>\n",
       "      <td>e s</td>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.148</td>\n",
       "      <td>64480</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>0.339821</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>495</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000002</td>\n",
       "      <td>1.528096e+09</td>\n",
       "      <td>e s</td>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.148</td>\n",
       "      <td>64481</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>0.339821</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>495</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000003</td>\n",
       "      <td>1.528096e+09</td>\n",
       "      <td>e s</td>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.148</td>\n",
       "      <td>64484</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>0.339821</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>495</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000004</td>\n",
       "      <td>1.528096e+09</td>\n",
       "      <td>e s</td>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.148</td>\n",
       "      <td>64485</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>0.339821</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>495</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000005</td>\n",
       "      <td>1.528096e+09</td>\n",
       "      <td>e s</td>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.148</td>\n",
       "      <td>64490</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>100</td>\n",
       "      <td>46</td>\n",
       "      <td>0.339821</td>\n",
       "      <td>0.326786</td>\n",
       "      <td>495</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>DDoS</td>\n",
       "      <td>TCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668517</th>\n",
       "      <td>3668518</td>\n",
       "      <td>1.529381e+09</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.150</td>\n",
       "      <td>35064</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>9.889330</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>455.754000</td>\n",
       "      <td>9.889330</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>Theft</td>\n",
       "      <td>Keylogging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668518</th>\n",
       "      <td>3668519</td>\n",
       "      <td>1.529381e+09</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.150</td>\n",
       "      <td>35066</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>9.889330</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>10453.000000</td>\n",
       "      <td>9.889330</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>Theft</td>\n",
       "      <td>Keylogging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668519</th>\n",
       "      <td>3668520</td>\n",
       "      <td>1.529381e+09</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.150</td>\n",
       "      <td>35070</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>9.889330</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>10.785200</td>\n",
       "      <td>9.889330</td>\n",
       "      <td>441</td>\n",
       "      <td>441</td>\n",
       "      <td>1</td>\n",
       "      <td>Theft</td>\n",
       "      <td>Keylogging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668520</th>\n",
       "      <td>3668521</td>\n",
       "      <td>1.529381e+09</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>1</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>43001</td>\n",
       "      <td>192.168.100.150</td>\n",
       "      <td>4433</td>\n",
       "      <td>...</td>\n",
       "      <td>666667.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>666667.000000</td>\n",
       "      <td>22346.400000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Theft</td>\n",
       "      <td>Keylogging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3668521</th>\n",
       "      <td>3668522</td>\n",
       "      <td>1.529381e+09</td>\n",
       "      <td>e</td>\n",
       "      <td>1</td>\n",
       "      <td>arp</td>\n",
       "      <td>2</td>\n",
       "      <td>192.168.100.3</td>\n",
       "      <td>-1</td>\n",
       "      <td>192.168.100.149</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Theft</td>\n",
       "      <td>Keylogging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3668522 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pkSeqID         stime flgs  flgs_number proto  proto_number  \\\n",
       "0        2000001  1.528096e+09  e s            2   tcp             1   \n",
       "1        2000002  1.528096e+09  e s            2   tcp             1   \n",
       "2        2000003  1.528096e+09  e s            2   tcp             1   \n",
       "3        2000004  1.528096e+09  e s            2   tcp             1   \n",
       "4        2000005  1.528096e+09  e s            2   tcp             1   \n",
       "...          ...           ...  ...          ...   ...           ...   \n",
       "3668517  3668518  1.529381e+09    e            1   tcp             1   \n",
       "3668518  3668519  1.529381e+09    e            1   tcp             1   \n",
       "3668519  3668520  1.529381e+09    e            1   tcp             1   \n",
       "3668520  3668521  1.529381e+09    e            1   tcp             1   \n",
       "3668521  3668522  1.529381e+09    e            1   arp             2   \n",
       "\n",
       "                   saddr  sport            daddr dport  ...  \\\n",
       "0        192.168.100.148  64480    192.168.100.3    80  ...   \n",
       "1        192.168.100.148  64481    192.168.100.3    80  ...   \n",
       "2        192.168.100.148  64484    192.168.100.3    80  ...   \n",
       "3        192.168.100.148  64485    192.168.100.3    80  ...   \n",
       "4        192.168.100.148  64490    192.168.100.3    80  ...   \n",
       "...                  ...    ...              ...   ...  ...   \n",
       "3668517  192.168.100.150  35064    192.168.100.3    22  ...   \n",
       "3668518  192.168.100.150  35066    192.168.100.3    22  ...   \n",
       "3668519  192.168.100.150  35070    192.168.100.3    22  ...   \n",
       "3668520    192.168.100.3  43001  192.168.100.150  4433  ...   \n",
       "3668521    192.168.100.3     -1  192.168.100.149    -1  ...   \n",
       "\n",
       "         AR_P_Proto_P_DstIP  N_IN_Conn_P_DstIP N_IN_Conn_P_SrcIP  \\\n",
       "0                  0.326786                100                46   \n",
       "1                  0.326786                100                46   \n",
       "2                  0.326786                100                46   \n",
       "3                  0.326786                100                46   \n",
       "4                  0.326786                100                46   \n",
       "...                     ...                ...               ...   \n",
       "3668517            9.889330                 19                19   \n",
       "3668518            9.889330                 19                19   \n",
       "3668519            9.889330                 19                19   \n",
       "3668520       666667.000000                  1                 3   \n",
       "3668521            0.018868                  2                 3   \n",
       "\n",
       "         AR_P_Proto_P_Sport  AR_P_Proto_P_Dport  \\\n",
       "0                  0.339821            0.326786   \n",
       "1                  0.339821            0.326786   \n",
       "2                  0.339821            0.326786   \n",
       "3                  0.339821            0.326786   \n",
       "4                  0.339821            0.326786   \n",
       "...                     ...                 ...   \n",
       "3668517          455.754000            9.889330   \n",
       "3668518        10453.000000            9.889330   \n",
       "3668519           10.785200            9.889330   \n",
       "3668520       666667.000000        22346.400000   \n",
       "3668521            0.018868            0.018868   \n",
       "\n",
       "         Pkts_P_State_P_Protocol_P_DestIP  Pkts_P_State_P_Protocol_P_SrcIP  \\\n",
       "0                                     495                              225   \n",
       "1                                     495                              225   \n",
       "2                                     495                              225   \n",
       "3                                     495                              225   \n",
       "4                                     495                              225   \n",
       "...                                   ...                              ...   \n",
       "3668517                                30                               30   \n",
       "3668518                                30                               30   \n",
       "3668519                               441                              441   \n",
       "3668520                                 2                                4   \n",
       "3668521                                16                               16   \n",
       "\n",
       "         attack  category  subcategory  \n",
       "0             1      DDoS          TCP  \n",
       "1             1      DDoS          TCP  \n",
       "2             1      DDoS          TCP  \n",
       "3             1      DDoS          TCP  \n",
       "4             1      DDoS          TCP  \n",
       "...         ...       ...          ...  \n",
       "3668517       1     Theft   Keylogging  \n",
       "3668518       1     Theft   Keylogging  \n",
       "3668519       1     Theft   Keylogging  \n",
       "3668520       1     Theft   Keylogging  \n",
       "3668521       1     Theft   Keylogging  \n",
       "\n",
       "[3668522 rows x 46 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e014d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(frame.columns)\n",
    "for i in ['pkSeqID', 'proto_number', 'saddr', 'sport', \n",
    "          'daddr', 'dport', 'flgs', 'flgs_number', 'category', 'subcategory']:\n",
    "    features.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d8d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical features\n",
    "cat_features = ['state', 'proto', 'state_number']\n",
    "for i in cat_features:\n",
    "    encoder = LabelEncoder()\n",
    "    frame[i] = encoder.fit_transform(frame[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18840fa3",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7b3ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for classification\n",
    "\n",
    "df = frame.dropna()\n",
    "\n",
    "x = df[features].to_numpy()\n",
    "y = df['attack'].to_numpy()\n",
    "\n",
    "# normalization min max\n",
    "scaler = MinMaxScaler()\n",
    "x = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff6fff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y,\n",
    "                                                                    train_size=0.80,\n",
    "                                                                    test_size=0.20,\n",
    "                                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a60aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [GaussianNB(), \n",
    "          DecisionTreeClassifier(criterion=\"entropy\",\n",
    "                                 class_weight=\"balanced\",\n",
    "                                 random_state=10,\n",
    "                                 max_depth=20,\n",
    "                                 max_leaf_nodes=162,\n",
    "                                 min_samples_leaf=20,\n",
    "                                 min_impurity_decrease=0.00006,\n",
    "                                 min_samples_split=2),\n",
    "          RandomForestClassifier(criterion=\"entropy\",\n",
    "                                 class_weight=\"balanced\",\n",
    "                                 random_state=10,\n",
    "                                 max_depth=20,\n",
    "                                 max_leaf_nodes=162,\n",
    "                                 min_samples_leaf=20,\n",
    "                                 min_impurity_decrease=0.00006,\n",
    "                                 min_samples_split=2,\n",
    "                                 n_estimators=75),\n",
    "          MLPClassifier(hidden_layer_sizes=(15,30,60),\n",
    "                        solver=\"adam\",\n",
    "                        activation=\"relu\",\n",
    "                        learning_rate_init=0.002,\n",
    "                        learning_rate=\"adaptive\",\n",
    "                        max_iter=2000\n",
    "                       ),\n",
    "          AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini',\n",
    "                                                                   random_state=10,\n",
    "                                                                   class_weight='balanced',\n",
    "                                                                   max_depth=11,\n",
    "                                                                   max_leaf_nodes=162,\n",
    "                                                                   min_samples_leaf=20,\n",
    "                                                                   min_impurity_decrease=0.00006),\n",
    "                            n_estimators=3300,\n",
    "                            learning_rate=0.3,\n",
    "                            algorithm='SAMME.R'),\n",
    "         GradientBoostingClassifier(loss='deviance',\n",
    "                                   n_estimators=3200,\n",
    "                                   learning_rate=0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8285bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_mlp = [MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(50,50),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(50,50,50),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(50,30,10),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(100,100),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(100,100,100),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(100,50,20),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(200,100, 50, 25),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(200,200,200),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(300,150,75),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               )\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d8a861",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_mlp_downsampling = [MLPClassifier(hidden_layer_sizes=(25,),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(25,25),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(25,25,25),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(25,15,10),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(15,),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(15,15),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(15,15,15),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(30,15, 5),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(5,5,5),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               )\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09deab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4bb6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_mlp_downsampling:\n",
    "    model.fit(x_train, y_train)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554fb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(hidden_layer_sizes=(50,), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(50, 50), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(50, 50, 50), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(50, 30, 10), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "for model in models_mlp:\n",
    "    model.fit(x_train, y_train)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bd238bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=20, max_leaf_nodes=162,\n",
      "                       min_impurity_decrease=6e-05, min_samples_leaf=20,\n",
      "                       random_state=10)\n",
      "RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=20, max_leaf_nodes=162,\n",
      "                       min_impurity_decrease=6e-05, min_samples_leaf=20,\n",
      "                       n_estimators=75, random_state=10)\n",
      "MLPClassifier(hidden_layer_sizes=(15, 30, 60), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced',\n",
      "                                                         max_depth=11,\n",
      "                                                         max_leaf_nodes=162,\n",
      "                                                         min_impurity_decrease=6e-05,\n",
      "                                                         min_samples_leaf=20,\n",
      "                                                         random_state=10),\n",
      "                   learning_rate=0.3, n_estimators=3300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mkubita/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_gb.py:310: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(learning_rate=0.05, loss='deviance',\n",
      "                           n_estimators=3200)\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "for model in models:\n",
    "    model.fit(x_train, y_train)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "18625ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368834, 39)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5146ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {i: None for i in models}\n",
    "for model, model_str in zip(models, predictions):\n",
    "    predictions[model_str] = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {i: None for i in models_mlp}\n",
    "for model, model_str in zip(models_mlp, predictions):\n",
    "    predictions[model_str] = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {i: None for i in models_mlp_downsampling}\n",
    "for model, model_str in zip(models_mlp_downsampling, predictions):\n",
    "    predictions[model_str] = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76398d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "# fpr_list = []\n",
    "for model, model_str in zip(models, predictions):\n",
    "    accuracy = accuracy_score(y_test, predictions[model_str])\n",
    "    precision = precision_score(y_test, predictions[model_str], average='binary')\n",
    "    recall = recall_score(y_test, predictions[model_str], average='binary')\n",
    "    f1_ = f1_score(y_test, predictions[model_str], average='binary')\n",
    "#     fpr_score = det_curve(y_test, predictions[model_str])[0][1]\n",
    "    \n",
    "    accuracy_list.append(round(accuracy, 3) * 100)\n",
    "    precision_list.append(round(precision,3) * 100)\n",
    "    recall_list.append(round(recall,3) * 100)\n",
    "#     fpr_list.append(round(fpr_score, 3) * 100)\n",
    "    f1_list.append(round(f1_,3) * 100)\n",
    "\n",
    "# accuracy_list[3] = 83.9\n",
    "# precision_list[3] = 82.4\n",
    "\n",
    "# accuracy_list.append(86.6)\n",
    "# accuracy_list.append(86.6)\n",
    "# precision_list.append(81.3)\n",
    "# recall_list.append(98.5)\n",
    "# f1_list.append(89.1)\n",
    "# precision_list.append(78.2)\n",
    "# recall_list.append(99.2)\n",
    "# f1_list.append(87.5)\n",
    "\n",
    "results = {'Model': models,\n",
    "           'Accuracy [%]': accuracy_list,\n",
    "           'Precision [%]': precision_list,\n",
    "           'Recall [%]': recall_list,\n",
    "           'F1_score [%]': f1_list,\n",
    "#            'FPR_score [%]': fpr_list\n",
    "          }\n",
    " \n",
    "# Convert the dictionary into DataFrame \n",
    "result_metrics = pd.DataFrame(results)\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "# fpr_list = []\n",
    "for model, model_str in zip(models_mlp, predictions):\n",
    "    accuracy = accuracy_score(y_test, predictions[model_str])\n",
    "    precision = precision_score(y_test, predictions[model_str], average='binary')\n",
    "    recall = recall_score(y_test, predictions[model_str], average='binary')\n",
    "    f1_ = f1_score(y_test, predictions[model_str], average='binary')\n",
    "#     fpr_score = det_curve(y_test, predictions[model_str])[0][1]\n",
    "    \n",
    "    accuracy_list.append(round(accuracy, 3) * 100)\n",
    "    precision_list.append(round(precision,3) * 100)\n",
    "    recall_list.append(round(recall,3) * 100)\n",
    "#     fpr_list.append(round(fpr_score, 3) * 100)\n",
    "    f1_list.append(round(f1_,3) * 100)\n",
    "\n",
    "# accuracy_list[3] = 83.9\n",
    "# precision_list[3] = 82.4\n",
    "\n",
    "# accuracy_list.append(86.6)\n",
    "# accuracy_list.append(86.6)\n",
    "# precision_list.append(81.3)\n",
    "# recall_list.append(98.5)\n",
    "# f1_list.append(89.1)\n",
    "# precision_list.append(78.2)\n",
    "# recall_list.append(99.2)\n",
    "# f1_list.append(87.5)\n",
    "\n",
    "results = {'Model': models_mlp,\n",
    "           'Accuracy [%]': accuracy_list,\n",
    "           'Precision [%]': precision_list,\n",
    "           'Recall [%]': recall_list,\n",
    "           'F1_score [%]': f1_list,\n",
    "#            'FPR_score [%]': fpr_list\n",
    "          }\n",
    " \n",
    "# Convert the dictionary into DataFrame \n",
    "result_metrics = pd.DataFrame(results)\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "# fpr_list = []\n",
    "for model, model_str in zip(models_mlp_downsampling, predictions):\n",
    "    accuracy = accuracy_score(y_test, predictions[model_str])\n",
    "    precision = precision_score(y_test, predictions[model_str], average='binary')\n",
    "    recall = recall_score(y_test, predictions[model_str], average='binary')\n",
    "    f1_ = f1_score(y_test, predictions[model_str], average='binary')\n",
    "    \n",
    "    accuracy_list.append(round(accuracy, 3) * 100)\n",
    "    precision_list.append(round(precision,3) * 100)\n",
    "    recall_list.append(round(recall,3) * 100)\n",
    "    f1_list.append(round(f1_,3) * 100)\n",
    "\n",
    "\n",
    "results = {'Model': models_mlp_downsampling,\n",
    "           'Accuracy [%]': accuracy_list,\n",
    "           'Precision [%]': precision_list,\n",
    "           'Recall [%]': recall_list,\n",
    "           'F1_score [%]': f1_list,\n",
    "#            'FPR_score [%]': fpr_list\n",
    "          }\n",
    " \n",
    "# Convert the dictionary into DataFrame \n",
    "result_metrics = pd.DataFrame(results)\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd74fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {i: None for i in models_mlp}\n",
    "for model, model_str in zip(models_mlp, predictions):\n",
    "    predictions[model_str] = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e45af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzic architektury MLP z mniejsza iloscia parametrów (downsampling)\n",
    "\n",
    "# AdaBoost, GBT -> testy\n",
    "\n",
    "# Inne zbiory danych\n",
    "\n",
    "\n",
    "# wazny element pracy -> dobrze opisany zbior danych\n",
    "# za tydzien piatek 11:30 24.06\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83fc568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0552466e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 00:02:46.669755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 00:02:46.670311: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 00:02:46.670415: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 00:02:46.670498: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 00:02:46.695391: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 00:02:46.695731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 00:02:46.695913: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-07 00:02:46.695950: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-07 00:02:46.697435: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(300, activation=\"tanh\", return_sequences = True, input_shape = (39, 1)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(200, activation=\"tanh\", return_sequences = True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(100, activation=\"tanh\", return_sequences = True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(80, activation=\"tanh\"))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b0e30ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5764/5764 [==============================] - 1059s 183ms/step - loss: 0.4214 - accuracy: 0.7530 - val_loss: 0.4253 - val_accuracy: 0.7012\n",
      "Epoch 2/20\n",
      "5764/5764 [==============================] - 1034s 179ms/step - loss: 0.3672 - accuracy: 0.8082 - val_loss: 0.5352 - val_accuracy: 0.6518\n",
      "Epoch 3/20\n",
      "5764/5764 [==============================] - 1085s 188ms/step - loss: 0.3649 - accuracy: 0.8029 - val_loss: 0.4680 - val_accuracy: 0.6724\n",
      "Epoch 4/20\n",
      "5764/5764 [==============================] - 1127s 196ms/step - loss: 0.2619 - accuracy: 0.8824 - val_loss: 0.1809 - val_accuracy: 0.9201\n",
      "Epoch 5/20\n",
      "5764/5764 [==============================] - 1195s 207ms/step - loss: 0.2678 - accuracy: 0.8700 - val_loss: 0.2246 - val_accuracy: 0.8881\n",
      "Epoch 6/20\n",
      "5764/5764 [==============================] - 1217s 211ms/step - loss: 0.2341 - accuracy: 0.8869 - val_loss: 0.1487 - val_accuracy: 0.9583\n",
      "Epoch 7/20\n",
      "5764/5764 [==============================] - 1219s 211ms/step - loss: 0.2149 - accuracy: 0.9064 - val_loss: 0.1800 - val_accuracy: 0.9106\n",
      "Epoch 8/20\n",
      "5764/5764 [==============================] - 1221s 212ms/step - loss: 0.1555 - accuracy: 0.9351 - val_loss: 0.1595 - val_accuracy: 0.9455\n",
      "Epoch 9/20\n",
      "5764/5764 [==============================] - 1222s 212ms/step - loss: 0.2285 - accuracy: 0.9016 - val_loss: 0.2694 - val_accuracy: 0.8701\n",
      "Epoch 10/20\n",
      "5764/5764 [==============================] - 1222s 212ms/step - loss: 0.2248 - accuracy: 0.8952 - val_loss: 0.1270 - val_accuracy: 0.9601\n",
      "Epoch 11/20\n",
      "5764/5764 [==============================] - 1224s 212ms/step - loss: 0.1978 - accuracy: 0.9057 - val_loss: 0.1567 - val_accuracy: 0.9339\n",
      "Epoch 12/20\n",
      "5764/5764 [==============================] - 1225s 213ms/step - loss: 0.2544 - accuracy: 0.8780 - val_loss: 0.2121 - val_accuracy: 0.8836\n",
      "Epoch 13/20\n",
      "5764/5764 [==============================] - 1227s 213ms/step - loss: 0.1746 - accuracy: 0.9307 - val_loss: 0.3007 - val_accuracy: 0.8451\n",
      "Epoch 14/20\n",
      "5764/5764 [==============================] - 1228s 213ms/step - loss: 0.1964 - accuracy: 0.9126 - val_loss: 0.1413 - val_accuracy: 0.9277\n",
      "Epoch 15/20\n",
      "5764/5764 [==============================] - 1222s 212ms/step - loss: 0.1509 - accuracy: 0.9388 - val_loss: 0.1360 - val_accuracy: 0.9464\n",
      "Epoch 16/20\n",
      "5764/5764 [==============================] - 1226s 213ms/step - loss: 0.2053 - accuracy: 0.9108 - val_loss: 0.1466 - val_accuracy: 0.9586\n",
      "Epoch 17/20\n",
      "5764/5764 [==============================] - 1229s 213ms/step - loss: 0.1790 - accuracy: 0.9255 - val_loss: 0.1747 - val_accuracy: 0.9110\n",
      "Epoch 18/20\n",
      "5764/5764 [==============================] - 1228s 213ms/step - loss: 0.1899 - accuracy: 0.9172 - val_loss: 0.2822 - val_accuracy: 0.8958\n",
      "Epoch 19/20\n",
      "5764/5764 [==============================] - 1230s 213ms/step - loss: 0.1601 - accuracy: 0.9372 - val_loss: 0.3011 - val_accuracy: 0.8736\n",
      "Epoch 20/20\n",
      "5764/5764 [==============================] - 1230s 213ms/step - loss: 0.1506 - accuracy: 0.9394 - val_loss: 0.0878 - val_accuracy: 0.9643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9dd90d3550>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, \n",
    "          epochs=20, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1d86edfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.964331030593543 0.9276747291883068 0.9734662099034569 0.9500189955170579\n"
     ]
    }
   ],
   "source": [
    "prediction_lstm = model.predict(x_test)\n",
    "prediction_lstm = prediction_lstm.flatten()\n",
    "prediction_lstm = [int(round(i,0)) for i in prediction_lstm]\n",
    "\n",
    "accuracy = accuracy_score(y_test, prediction_lstm)\n",
    "precision = precision_score(y_test, prediction_lstm, average='binary')\n",
    "recall = recall_score(y_test, prediction_lstm, average='binary')\n",
    "f1_ = f1_score(y_test, prediction_lstm, average='binary')\n",
    "\n",
    "print(accuracy, precision, recall, f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a021f00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 09:22:27.441232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 09:22:27.441581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 09:22:27.441644: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 09:22:27.441699: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 09:22:27.468290: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 09:22:27.468545: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 09:22:27.468780: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-07 09:22:27.468810: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-07 09:22:27.470300: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_GRU = Sequential()\n",
    "\n",
    "model_GRU.add(GRU(300, activation=\"tanh\", return_sequences = True, input_shape = (39, 1)))\n",
    "model_GRU.add(Dropout(0.4))\n",
    "\n",
    "model_GRU.add(GRU(200, activation=\"tanh\", return_sequences = True))\n",
    "model_GRU.add(Dropout(0.4))\n",
    "\n",
    "model_GRU.add(GRU(100, activation=\"tanh\", return_sequences = True))\n",
    "model_GRU.add(Dropout(0.4))\n",
    "\n",
    "model_GRU.add(GRU(80, activation=\"tanh\"))\n",
    "model_GRU.add(Dropout(0.4))\n",
    "\n",
    "model_GRU.add(Dense(1))\n",
    "model_GRU.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_GRU.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b716ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5764/5764 [==============================] - 1009s 174ms/step - loss: 0.2773 - accuracy: 0.8680 - val_loss: 0.2482 - val_accuracy: 0.8810\n",
      "Epoch 2/20\n",
      "5764/5764 [==============================] - 1070s 186ms/step - loss: 0.1640 - accuracy: 0.9324 - val_loss: 0.1240 - val_accuracy: 0.9491\n",
      "Epoch 3/20\n",
      "5764/5764 [==============================] - 1022s 177ms/step - loss: 0.1224 - accuracy: 0.9532 - val_loss: 0.3028 - val_accuracy: 0.8785\n",
      "Epoch 4/20\n",
      "5764/5764 [==============================] - 1104s 192ms/step - loss: 0.1829 - accuracy: 0.9287 - val_loss: 0.1325 - val_accuracy: 0.9567\n",
      "Epoch 5/20\n",
      "5764/5764 [==============================] - 1098s 190ms/step - loss: 0.1669 - accuracy: 0.9346 - val_loss: 0.1297 - val_accuracy: 0.9444\n",
      "Epoch 6/20\n",
      "5764/5764 [==============================] - 1115s 194ms/step - loss: 0.1689 - accuracy: 0.9318 - val_loss: 0.1907 - val_accuracy: 0.8768\n",
      "Epoch 7/20\n",
      "5764/5764 [==============================] - 1119s 194ms/step - loss: 0.1453 - accuracy: 0.9452 - val_loss: 0.1332 - val_accuracy: 0.9560\n",
      "Epoch 8/20\n",
      "5764/5764 [==============================] - 1140s 198ms/step - loss: 0.1534 - accuracy: 0.9409 - val_loss: 0.1177 - val_accuracy: 0.9585\n",
      "Epoch 9/20\n",
      "5764/5764 [==============================] - 1120s 194ms/step - loss: 0.1541 - accuracy: 0.9397 - val_loss: 0.1478 - val_accuracy: 0.9260\n",
      "Epoch 10/20\n",
      "5764/5764 [==============================] - 1101s 191ms/step - loss: 0.1509 - accuracy: 0.9425 - val_loss: 0.1217 - val_accuracy: 0.9592\n",
      "Epoch 11/20\n",
      "5764/5764 [==============================] - 1065s 185ms/step - loss: 0.1325 - accuracy: 0.9505 - val_loss: 0.1262 - val_accuracy: 0.9576\n",
      "Epoch 12/20\n",
      "5764/5764 [==============================] - 1154s 200ms/step - loss: 0.1500 - accuracy: 0.9435 - val_loss: 0.1280 - val_accuracy: 0.9615\n",
      "Epoch 13/20\n",
      "5764/5764 [==============================] - 1167s 202ms/step - loss: 0.1410 - accuracy: 0.9473 - val_loss: 0.1133 - val_accuracy: 0.9613\n",
      "Epoch 14/20\n",
      "5764/5764 [==============================] - 1164s 202ms/step - loss: 0.1407 - accuracy: 0.9470 - val_loss: 0.1054 - val_accuracy: 0.9604\n",
      "Epoch 15/20\n",
      "5764/5764 [==============================] - 1160s 201ms/step - loss: 0.1451 - accuracy: 0.9460 - val_loss: 0.1303 - val_accuracy: 0.9570\n",
      "Epoch 16/20\n",
      "5764/5764 [==============================] - 1056s 183ms/step - loss: 0.1762 - accuracy: 0.9278 - val_loss: 0.1455 - val_accuracy: 0.9641\n",
      "Epoch 17/20\n",
      "5764/5764 [==============================] - 1113s 193ms/step - loss: 0.1376 - accuracy: 0.9481 - val_loss: 0.1120 - val_accuracy: 0.9625\n",
      "Epoch 18/20\n",
      "5764/5764 [==============================] - 1099s 191ms/step - loss: 0.1452 - accuracy: 0.9473 - val_loss: 0.1220 - val_accuracy: 0.9601\n",
      "Epoch 19/20\n",
      "5764/5764 [==============================] - 1138s 197ms/step - loss: 0.2115 - accuracy: 0.9138 - val_loss: 0.2085 - val_accuracy: 0.9208\n",
      "Epoch 20/20\n",
      "5764/5764 [==============================] - 1146s 199ms/step - loss: 0.1809 - accuracy: 0.9264 - val_loss: 0.2169 - val_accuracy: 0.9238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd96e1948e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU.fit(x_train, y_train, batch_size=64, \n",
    "          epochs=20, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02174530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9238469129911397 0.8411105672485996 0.9632824665213329 0.8980605075198885\n"
     ]
    }
   ],
   "source": [
    "prediction_gru = model_GRU.predict(x_test)\n",
    "prediction_gru = prediction_gru.flatten()\n",
    "prediction_gru = [int(round(i,0)) for i in prediction_gru]\n",
    "\n",
    "accuracy = accuracy_score(y_test, prediction_gru)\n",
    "precision = precision_score(y_test, prediction_gru, average='binary')\n",
    "recall = recall_score(y_test, prediction_gru, average='binary')\n",
    "f1_ = f1_score(y_test, prediction_gru, average='binary')\n",
    "\n",
    "print(accuracy, precision, recall, f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585d560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f262cfe4",
   "metadata": {},
   "source": [
    "## Reduced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "991d190e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_features = ['ts', 'proto', 'src_ip_bytes', 'src_pkts', 'dst_ip_bytes', 'dst_pkts', 'conn_state', 'dst_bytes', 'src_bytes', 'duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2106654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for classification\n",
    "\n",
    "x_reduced = df[features][reduced_features].to_numpy()\n",
    "y_reduced = df['label'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a25e12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization min max\n",
    "scaler = MinMaxScaler()\n",
    "x_reduced = scaler.fit_transform(x_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a36e043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "x_reduced_train, x_reduced_test, y_reduced_train, y_reduced_test = model_selection.train_test_split(x_reduced, y_reduced,\n",
    "                                                                    train_size=0.80,\n",
    "                                                                    test_size=0.20,\n",
    "                                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fc2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_reduced = [GaussianNB(), \n",
    "          DecisionTreeClassifier(criterion=\"entropy\",\n",
    "                                 class_weight=\"balanced\",\n",
    "                                 random_state=10,\n",
    "                                 max_depth=20,\n",
    "                                 max_leaf_nodes=162,\n",
    "                                 min_samples_leaf=20,\n",
    "                                 min_impurity_decrease=0.00006,\n",
    "                                 min_samples_split=2),\n",
    "          RandomForestClassifier(criterion=\"entropy\",\n",
    "                                 class_weight=\"balanced\",\n",
    "                                 random_state=10,\n",
    "                                 max_depth=20,\n",
    "                                 max_leaf_nodes=162,\n",
    "                                 min_samples_leaf=20,\n",
    "                                 min_impurity_decrease=0.00006,\n",
    "                                 min_samples_split=2,\n",
    "                                 n_estimators=75),\n",
    "          MLPClassifier(hidden_layer_sizes=(15,30,60),\n",
    "                        solver=\"adam\",\n",
    "                        activation=\"relu\",\n",
    "                        learning_rate_init=0.002,\n",
    "                        learning_rate=\"adaptive\",\n",
    "                        max_iter=2000\n",
    "                       ),\n",
    "          AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion='gini',\n",
    "                                                                   random_state=10,\n",
    "                                                                   class_weight='balanced',\n",
    "                                                                   max_depth=11,\n",
    "                                                                   max_leaf_nodes=162,\n",
    "                                                                   min_samples_leaf=20,\n",
    "                                                                   min_impurity_decrease=0.00006),\n",
    "                            n_estimators=3300,\n",
    "                            learning_rate=0.3,\n",
    "                            algorithm='SAMME.R'),\n",
    "         GradientBoostingClassifier(loss='deviance',\n",
    "                                   n_estimators=3200,\n",
    "                                   learning_rate=0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faacf61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_mlp = [MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(50,50),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(50,50,50),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(50,30,10),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(100,100),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(100,100,100),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(100,50,20),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(200,100, 50, 25),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(200,200,200),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(300,150,75),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               )\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d39c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_mlp_downsampling = [MLPClassifier(hidden_layer_sizes=(25,),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(25,25),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(25,25,25),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(25,15,10),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(15,),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(15,15),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(15,15,15),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "             MLPClassifier(hidden_layer_sizes=(30,15, 5),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               ),\n",
    "            MLPClassifier(hidden_layer_sizes=(5,5,5),\n",
    "                solver=\"adam\",\n",
    "                activation=\"relu\",\n",
    "                learning_rate_init=0.002,\n",
    "                learning_rate=\"adaptive\",\n",
    "                max_iter=2000\n",
    "               )\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe33849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(hidden_layer_sizes=(25,), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(25, 25), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(25, 25, 25), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(25, 15, 10), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(15,), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(15, 15), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(15, 15, 15), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(30, 15, 5), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(5, 5, 5), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "for model in models_mlp_downsampling:\n",
    "    model.fit(x_reduced_train, y_reduced_train)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d0cff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(hidden_layer_sizes=(50,), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(50, 50), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(50, 50, 50), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(50, 30, 10), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(100, 100), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(100, 100, 100), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(100, 50, 20), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(200, 100, 50, 25), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(200, 200, 200), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "MLPClassifier(hidden_layer_sizes=(300, 150, 75), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "for model in models_mlp:\n",
    "    model.fit(x_reduced_train, y_reduced_train)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5587e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = {i: None for i in models_mlp}\n",
    "for model, model_str in zip(models_mlp, predictions_2):\n",
    "    predictions_2[model_str] = model.predict(x_reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "5df55650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB()\n",
      "DecisionTreeClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=20, max_leaf_nodes=162,\n",
      "                       min_impurity_decrease=6e-05, min_samples_leaf=20,\n",
      "                       random_state=10)\n",
      "RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=20, max_leaf_nodes=162,\n",
      "                       min_impurity_decrease=6e-05, min_samples_leaf=20,\n",
      "                       n_estimators=75, random_state=10)\n",
      "MLPClassifier(hidden_layer_sizes=(15, 30, 60), learning_rate='adaptive',\n",
      "              learning_rate_init=0.002, max_iter=2000)\n",
      "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(class_weight='balanced',\n",
      "                                                         max_depth=11,\n",
      "                                                         max_leaf_nodes=162,\n",
      "                                                         min_impurity_decrease=6e-05,\n",
      "                                                         min_samples_leaf=20,\n",
      "                                                         random_state=10),\n",
      "                   learning_rate=0.3, n_estimators=3300)\n",
      "GradientBoostingClassifier(learning_rate=0.05, n_estimators=3200)\n"
     ]
    }
   ],
   "source": [
    "# train models\n",
    "for model in models_reduced:\n",
    "    model.fit(x_reduced_train, y_reduced_train)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "83c7ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = {i: None for i in models_reduced}\n",
    "for model, model_str in zip(models_reduced, predictions_2):\n",
    "    predictions_2[model_str] = model.predict(x_reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cd1f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "predictions_2 = {i: None for i in models_mlp_downsampling}\n",
    "for model, model_str in zip(models_mlp_downsampling, predictions_2):\n",
    "    predictions_2[model_str] = model.predict(x_reduced_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "905ce850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy [%]</th>\n",
       "      <th>Precision [%]</th>\n",
       "      <th>Recall [%]</th>\n",
       "      <th>F1_score [%]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>87.3</td>\n",
       "      <td>75.7</td>\n",
       "      <td>93.5</td>\n",
       "      <td>83.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>99.9</td>\n",
       "      <td>99.9</td>\n",
       "      <td>100.0</td>\n",
       "      <td>99.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>99.8</td>\n",
       "      <td>99.6</td>\n",
       "      <td>99.8</td>\n",
       "      <td>99.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MultiLayerPercepton</td>\n",
       "      <td>96.8</td>\n",
       "      <td>92.9</td>\n",
       "      <td>98.3</td>\n",
       "      <td>95.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GradientBoost</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy [%]  Precision [%]  Recall [%]  F1_score [%]\n",
       "0           NaiveBayes          87.3           75.7        93.5          83.6\n",
       "1         DecisionTree          99.9           99.9       100.0          99.9\n",
       "2         RandomForest          99.8           99.6        99.8          99.7\n",
       "3  MultiLayerPercepton          96.8           92.9        98.3          95.5\n",
       "4             AdaBoost         100.0          100.0       100.0         100.0\n",
       "5        GradientBoost         100.0          100.0       100.0         100.0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "# fpr_list = []\n",
    "for model, model_str in zip(models_reduced, predictions_2):\n",
    "    accuracy = accuracy_score(y_reduced_test, predictions_2[model_str])\n",
    "    precision = precision_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    recall = recall_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    f1_ = f1_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    \n",
    "    accuracy_list.append(round(accuracy, 3) * 100)\n",
    "    precision_list.append(round(precision,3) * 100)\n",
    "    recall_list.append(round(recall,3) * 100)\n",
    "    f1_list.append(round(f1_,3) * 100)\n",
    "\n",
    "# accuracy_list[3] = 83.9\n",
    "# precision_list[3] = 82.4\n",
    "\n",
    "# accuracy_list.append(86.6)\n",
    "# accuracy_list.append(86.6)\n",
    "# precision_list.append(81.3)\n",
    "# recall_list.append(98.5)\n",
    "# f1_list.append(89.1)\n",
    "# precision_list.append(78.2)\n",
    "# recall_list.append(99.2)\n",
    "# f1_list.append(87.5)\n",
    "\n",
    "results = {'Model': ['NaiveBayes', 'DecisionTree', 'RandomForest', 'MultiLayerPercepton', 'AdaBoost', 'GradientBoost'],\n",
    "           'Accuracy [%]': accuracy_list,\n",
    "           'Precision [%]': precision_list,\n",
    "           'Recall [%]': recall_list,\n",
    "           'F1_score [%]': f1_list,\n",
    "#            'FPR_score [%]': fpr_list\n",
    "          }\n",
    " \n",
    "# Convert the dictionary into DataFrame \n",
    "result_metrics = pd.DataFrame(results)\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0d42057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy [%]</th>\n",
       "      <th>Precision [%]</th>\n",
       "      <th>Recall [%]</th>\n",
       "      <th>F1_score [%]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(50,), learni...</td>\n",
       "      <td>95.8</td>\n",
       "      <td>90.8</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(50, 50), lea...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>98.4</td>\n",
       "      <td>95.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(50, 50, 50),...</td>\n",
       "      <td>97.1</td>\n",
       "      <td>93.8</td>\n",
       "      <td>98.2</td>\n",
       "      <td>95.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(50, 30, 10),...</td>\n",
       "      <td>92.1</td>\n",
       "      <td>82.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>89.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(100, 100), l...</td>\n",
       "      <td>96.9</td>\n",
       "      <td>93.2</td>\n",
       "      <td>98.2</td>\n",
       "      <td>95.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(100, 100, 10...</td>\n",
       "      <td>96.6</td>\n",
       "      <td>92.4</td>\n",
       "      <td>98.3</td>\n",
       "      <td>95.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(100, 50, 20)...</td>\n",
       "      <td>94.4</td>\n",
       "      <td>87.2</td>\n",
       "      <td>98.3</td>\n",
       "      <td>92.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(200, 100, 50...</td>\n",
       "      <td>96.3</td>\n",
       "      <td>91.6</td>\n",
       "      <td>98.3</td>\n",
       "      <td>94.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(200, 200, 20...</td>\n",
       "      <td>92.7</td>\n",
       "      <td>94.1</td>\n",
       "      <td>84.2</td>\n",
       "      <td>88.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(300, 150, 75...</td>\n",
       "      <td>92.5</td>\n",
       "      <td>94.3</td>\n",
       "      <td>83.4</td>\n",
       "      <td>88.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy [%]  \\\n",
       "0  MLPClassifier(hidden_layer_sizes=(50,), learni...          95.8   \n",
       "1  MLPClassifier(hidden_layer_sizes=(50, 50), lea...          97.0   \n",
       "2  MLPClassifier(hidden_layer_sizes=(50, 50, 50),...          97.1   \n",
       "3  MLPClassifier(hidden_layer_sizes=(50, 30, 10),...          92.1   \n",
       "4  MLPClassifier(hidden_layer_sizes=(100, 100), l...          96.9   \n",
       "5  MLPClassifier(hidden_layer_sizes=(100, 100, 10...          96.6   \n",
       "6  MLPClassifier(hidden_layer_sizes=(100, 50, 20)...          94.4   \n",
       "7  MLPClassifier(hidden_layer_sizes=(200, 100, 50...          96.3   \n",
       "8  MLPClassifier(hidden_layer_sizes=(200, 200, 20...          92.7   \n",
       "9  MLPClassifier(hidden_layer_sizes=(300, 150, 75...          92.5   \n",
       "\n",
       "   Precision [%]  Recall [%]  F1_score [%]  \n",
       "0           90.8        98.0          94.3  \n",
       "1           93.4        98.4          95.8  \n",
       "2           93.8        98.2          95.9  \n",
       "3           82.0        99.0          89.7  \n",
       "4           93.2        98.2          95.7  \n",
       "5           92.4        98.3          95.3  \n",
       "6           87.2        98.3          92.4  \n",
       "7           91.6        98.3          94.8  \n",
       "8           94.1        84.2          88.9  \n",
       "9           94.3        83.4          88.5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "# fpr_list = []\n",
    "for model, model_str in zip(models_mlp, predictions_2):\n",
    "    accuracy = accuracy_score(y_reduced_test, predictions_2[model_str])\n",
    "    precision = precision_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    recall = recall_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    f1_ = f1_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    \n",
    "    accuracy_list.append(round(accuracy, 3) * 100)\n",
    "    precision_list.append(round(precision,3) * 100)\n",
    "    recall_list.append(round(recall,3) * 100)\n",
    "    f1_list.append(round(f1_,3) * 100)\n",
    "\n",
    "# accuracy_list[3] = 83.9\n",
    "# precision_list[3] = 82.4\n",
    "\n",
    "# accuracy_list.append(86.6)\n",
    "# accuracy_list.append(86.6)\n",
    "# precision_list.append(81.3)\n",
    "# recall_list.append(98.5)\n",
    "# f1_list.append(89.1)\n",
    "# precision_list.append(78.2)\n",
    "# recall_list.append(99.2)\n",
    "# f1_list.append(87.5)\n",
    "\n",
    "results = {'Model': models_mlp,\n",
    "           'Accuracy [%]': accuracy_list,\n",
    "           'Precision [%]': precision_list,\n",
    "           'Recall [%]': recall_list,\n",
    "           'F1_score [%]': f1_list,\n",
    "#            'FPR_score [%]': fpr_list\n",
    "          }\n",
    " \n",
    "# Convert the dictionary into DataFrame \n",
    "result_metrics = pd.DataFrame(results)\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9a9ae53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy [%]</th>\n",
       "      <th>Precision [%]</th>\n",
       "      <th>Recall [%]</th>\n",
       "      <th>F1_score [%]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(25,), learni...</td>\n",
       "      <td>95.7</td>\n",
       "      <td>90.3</td>\n",
       "      <td>98.0</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(25, 25), lea...</td>\n",
       "      <td>96.9</td>\n",
       "      <td>93.3</td>\n",
       "      <td>98.3</td>\n",
       "      <td>95.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(25, 25, 25),...</td>\n",
       "      <td>96.8</td>\n",
       "      <td>93.1</td>\n",
       "      <td>98.1</td>\n",
       "      <td>95.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(25, 15, 10),...</td>\n",
       "      <td>96.9</td>\n",
       "      <td>93.2</td>\n",
       "      <td>98.3</td>\n",
       "      <td>95.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(15,), learni...</td>\n",
       "      <td>93.7</td>\n",
       "      <td>94.3</td>\n",
       "      <td>87.1</td>\n",
       "      <td>90.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(15, 15), lea...</td>\n",
       "      <td>96.6</td>\n",
       "      <td>97.1</td>\n",
       "      <td>93.1</td>\n",
       "      <td>95.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(15, 15, 15),...</td>\n",
       "      <td>97.3</td>\n",
       "      <td>96.8</td>\n",
       "      <td>95.3</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(30, 15, 5), ...</td>\n",
       "      <td>96.8</td>\n",
       "      <td>92.9</td>\n",
       "      <td>98.3</td>\n",
       "      <td>95.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLPClassifier(hidden_layer_sizes=(5, 5, 5), le...</td>\n",
       "      <td>87.0</td>\n",
       "      <td>75.2</td>\n",
       "      <td>93.5</td>\n",
       "      <td>83.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Model  Accuracy [%]  \\\n",
       "0  MLPClassifier(hidden_layer_sizes=(25,), learni...          95.7   \n",
       "1  MLPClassifier(hidden_layer_sizes=(25, 25), lea...          96.9   \n",
       "2  MLPClassifier(hidden_layer_sizes=(25, 25, 25),...          96.8   \n",
       "3  MLPClassifier(hidden_layer_sizes=(25, 15, 10),...          96.9   \n",
       "4  MLPClassifier(hidden_layer_sizes=(15,), learni...          93.7   \n",
       "5  MLPClassifier(hidden_layer_sizes=(15, 15), lea...          96.6   \n",
       "6  MLPClassifier(hidden_layer_sizes=(15, 15, 15),...          97.3   \n",
       "7  MLPClassifier(hidden_layer_sizes=(30, 15, 5), ...          96.8   \n",
       "8  MLPClassifier(hidden_layer_sizes=(5, 5, 5), le...          87.0   \n",
       "\n",
       "   Precision [%]  Recall [%]  F1_score [%]  \n",
       "0           90.3        98.0          94.0  \n",
       "1           93.3        98.3          95.7  \n",
       "2           93.1        98.1          95.6  \n",
       "3           93.2        98.3          95.7  \n",
       "4           94.3        87.1          90.6  \n",
       "5           97.1        93.1          95.1  \n",
       "6           96.8        95.3          96.0  \n",
       "7           92.9        98.3          95.5  \n",
       "8           75.2        93.5          83.3  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\n",
    "accuracy_list, precision_list, recall_list, f1_list = [], [], [], []\n",
    "# fpr_list = []\n",
    "for model, model_str in zip(models_mlp_downsampling, predictions_2):\n",
    "    accuracy = accuracy_score(y_reduced_test, predictions_2[model_str])\n",
    "    precision = precision_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    recall = recall_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    f1_ = f1_score(y_reduced_test, predictions_2[model_str], average='binary')\n",
    "    \n",
    "    accuracy_list.append(round(accuracy, 3) * 100)\n",
    "    precision_list.append(round(precision,3) * 100)\n",
    "    recall_list.append(round(recall,3) * 100)\n",
    "    f1_list.append(round(f1_,3) * 100)\n",
    "\n",
    "# accuracy_list[3] = 83.9\n",
    "# precision_list[3] = 82.4\n",
    "\n",
    "# accuracy_list.append(86.6)\n",
    "# accuracy_list.append(86.6)\n",
    "# precision_list.append(81.3)\n",
    "# recall_list.append(98.5)\n",
    "# f1_list.append(89.1)\n",
    "# precision_list.append(78.2)\n",
    "# recall_list.append(99.2)\n",
    "# f1_list.append(87.5)\n",
    "\n",
    "results = {'Model': models_mlp_downsampling,\n",
    "           'Accuracy [%]': accuracy_list,\n",
    "           'Precision [%]': precision_list,\n",
    "           'Recall [%]': recall_list,\n",
    "           'F1_score [%]': f1_list,\n",
    "#            'FPR_score [%]': fpr_list\n",
    "          }\n",
    " \n",
    "# Convert the dictionary into DataFrame \n",
    "result_metrics = pd.DataFrame(results)\n",
    "result_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e85489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "607fcf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 18:55:56.176668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 18:55:56.177032: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 18:55:56.177101: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 18:55:56.177160: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 18:55:56.197804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 18:55:56.197868: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 18:55:56.197926: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-07 18:55:56.197935: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-07 18:55:56.198773: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(LSTM(300, activation=\"tanh\", return_sequences = True, input_shape = (10, 1)))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "model2.add(LSTM(200, activation=\"tanh\", return_sequences = True))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "model2.add(LSTM(100, activation=\"tanh\", return_sequences = True))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "model2.add(LSTM(80, activation=\"tanh\"))\n",
    "model2.add(Dropout(0.4))\n",
    "\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "400cceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5764/5764 [==============================] - 320s 55ms/step - loss: 0.2869 - accuracy: 0.8644 - val_loss: 0.2589 - val_accuracy: 0.8711\n",
      "Epoch 2/20\n",
      "5764/5764 [==============================] - 319s 55ms/step - loss: 0.2633 - accuracy: 0.8713 - val_loss: 0.2600 - val_accuracy: 0.9015\n",
      "Epoch 3/20\n",
      "5764/5764 [==============================] - 323s 56ms/step - loss: 0.2525 - accuracy: 0.8734 - val_loss: 0.1842 - val_accuracy: 0.8858\n",
      "Epoch 4/20\n",
      "5764/5764 [==============================] - 332s 58ms/step - loss: 0.2428 - accuracy: 0.8739 - val_loss: 0.1838 - val_accuracy: 0.9013\n",
      "Epoch 5/20\n",
      "5764/5764 [==============================] - 315s 55ms/step - loss: 0.2147 - accuracy: 0.8910 - val_loss: 0.1563 - val_accuracy: 0.9220\n",
      "Epoch 6/20\n",
      "5764/5764 [==============================] - 315s 55ms/step - loss: 0.1869 - accuracy: 0.9074 - val_loss: 0.3346 - val_accuracy: 0.8716\n",
      "Epoch 7/20\n",
      "5764/5764 [==============================] - 317s 55ms/step - loss: 0.2280 - accuracy: 0.8867 - val_loss: 0.2700 - val_accuracy: 0.8715\n",
      "Epoch 8/20\n",
      "5764/5764 [==============================] - 315s 55ms/step - loss: 0.2111 - accuracy: 0.8954 - val_loss: 0.2492 - val_accuracy: 0.8728\n",
      "Epoch 9/20\n",
      "5764/5764 [==============================] - 315s 55ms/step - loss: 0.2251 - accuracy: 0.8892 - val_loss: 0.3100 - val_accuracy: 0.8688\n",
      "Epoch 10/20\n",
      "5764/5764 [==============================] - 322s 56ms/step - loss: 0.2752 - accuracy: 0.8744 - val_loss: 0.3186 - val_accuracy: 0.8688\n",
      "Epoch 11/20\n",
      "5764/5764 [==============================] - 317s 55ms/step - loss: 0.2594 - accuracy: 0.8837 - val_loss: 0.3202 - val_accuracy: 0.8688\n",
      "Epoch 12/20\n",
      "5764/5764 [==============================] - 322s 56ms/step - loss: 0.2815 - accuracy: 0.8791 - val_loss: 0.2003 - val_accuracy: 0.8887\n",
      "Epoch 13/20\n",
      "5764/5764 [==============================] - 318s 55ms/step - loss: 0.2299 - accuracy: 0.8911 - val_loss: 0.1849 - val_accuracy: 0.9087\n",
      "Epoch 14/20\n",
      "5764/5764 [==============================] - 323s 56ms/step - loss: 0.2213 - accuracy: 0.8954 - val_loss: 0.2998 - val_accuracy: 0.8715\n",
      "Epoch 15/20\n",
      "5764/5764 [==============================] - 319s 55ms/step - loss: 0.2187 - accuracy: 0.8994 - val_loss: 0.1660 - val_accuracy: 0.9220\n",
      "Epoch 16/20\n",
      "5764/5764 [==============================] - 319s 55ms/step - loss: 0.2226 - accuracy: 0.8949 - val_loss: 0.1661 - val_accuracy: 0.9220\n",
      "Epoch 17/20\n",
      "5764/5764 [==============================] - 324s 56ms/step - loss: 0.2159 - accuracy: 0.8996 - val_loss: 0.1720 - val_accuracy: 0.9220\n",
      "Epoch 18/20\n",
      "5764/5764 [==============================] - 340s 59ms/step - loss: 0.2071 - accuracy: 0.9042 - val_loss: 0.2981 - val_accuracy: 0.8715\n",
      "Epoch 19/20\n",
      "5764/5764 [==============================] - 344s 60ms/step - loss: 0.2284 - accuracy: 0.8890 - val_loss: 0.2173 - val_accuracy: 0.8860\n",
      "Epoch 20/20\n",
      "5764/5764 [==============================] - 338s 59ms/step - loss: 0.2199 - accuracy: 0.8931 - val_loss: 0.2281 - val_accuracy: 0.8758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3fcb809fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x_reduced_train, y_reduced_train, batch_size=64, \n",
    "          epochs=20, validation_data=(x_reduced_test, y_reduced_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "080a43a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8757713455302628 0.7533423279774316 0.9563998754282155 0.8428130360205832\n"
     ]
    }
   ],
   "source": [
    "prediction2 = model2.predict(x_reduced_test)\n",
    "prediction2 = prediction2.flatten()\n",
    "prediction2 = [int(round(i,0)) for i in prediction2]\n",
    "\n",
    "accuracy = accuracy_score(y_reduced_test, prediction2)\n",
    "precision = precision_score(y_reduced_test, prediction2, average='binary')\n",
    "recall = recall_score(y_reduced_test, prediction2, average='binary')\n",
    "f1_ = f1_score(y_reduced_test, prediction2, average='binary')\n",
    "\n",
    "print(accuracy, precision, recall, f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0df431ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GRU2 = Sequential()\n",
    "\n",
    "model_GRU2.add(GRU(300, activation=\"tanh\", return_sequences = True, input_shape = (10, 1)))\n",
    "model_GRU2.add(Dropout(0.4))\n",
    "\n",
    "model_GRU2.add(GRU(200, activation=\"tanh\", return_sequences = True))\n",
    "model_GRU2.add(Dropout(0.4))\n",
    "\n",
    "model_GRU2.add(GRU(100, activation=\"tanh\", return_sequences = True))\n",
    "model_GRU2.add(Dropout(0.4))\n",
    "\n",
    "model_GRU2.add(GRU(80, activation=\"tanh\"))\n",
    "model_GRU2.add(Dropout(0.4))\n",
    "\n",
    "model_GRU2.add(Dense(1))\n",
    "model_GRU2.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_GRU2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4cbb1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5764/5764 [==============================] - 278s 47ms/step - loss: 0.2723 - accuracy: 0.8735 - val_loss: 0.1480 - val_accuracy: 0.9571\n",
      "Epoch 2/20\n",
      "5764/5764 [==============================] - 274s 47ms/step - loss: 0.1601 - accuracy: 0.9328 - val_loss: 0.0948 - val_accuracy: 0.9670\n",
      "Epoch 3/20\n",
      "5764/5764 [==============================] - 278s 48ms/step - loss: 0.1256 - accuracy: 0.9491 - val_loss: 0.1671 - val_accuracy: 0.9411\n",
      "Epoch 4/20\n",
      "5764/5764 [==============================] - 303s 53ms/step - loss: 0.1628 - accuracy: 0.9384 - val_loss: 0.1168 - val_accuracy: 0.9589\n",
      "Epoch 5/20\n",
      "5764/5764 [==============================] - 302s 52ms/step - loss: 0.1575 - accuracy: 0.9385 - val_loss: 0.3414 - val_accuracy: 0.8685\n",
      "Epoch 6/20\n",
      "5764/5764 [==============================] - 302s 52ms/step - loss: 0.2393 - accuracy: 0.8849 - val_loss: 0.2939 - val_accuracy: 0.8728\n",
      "Epoch 7/20\n",
      "5764/5764 [==============================] - 292s 51ms/step - loss: 0.2606 - accuracy: 0.8775 - val_loss: 0.2069 - val_accuracy: 0.8874\n",
      "Epoch 8/20\n",
      "5764/5764 [==============================] - 310s 54ms/step - loss: 0.2560 - accuracy: 0.8837 - val_loss: 0.1682 - val_accuracy: 0.9280\n",
      "Epoch 9/20\n",
      "5764/5764 [==============================] - 308s 53ms/step - loss: 0.1992 - accuracy: 0.9054 - val_loss: 0.1730 - val_accuracy: 0.9059\n",
      "Epoch 10/20\n",
      "5764/5764 [==============================] - 307s 53ms/step - loss: 0.2170 - accuracy: 0.8919 - val_loss: 0.2884 - val_accuracy: 0.8688\n",
      "Epoch 11/20\n",
      "5764/5764 [==============================] - 322s 56ms/step - loss: 0.2259 - accuracy: 0.8897 - val_loss: 0.2153 - val_accuracy: 0.8860\n",
      "Epoch 12/20\n",
      "5764/5764 [==============================] - 339s 59ms/step - loss: 0.2947 - accuracy: 0.8624 - val_loss: 0.2369 - val_accuracy: 0.8989\n",
      "Epoch 13/20\n",
      "5764/5764 [==============================] - 309s 54ms/step - loss: 0.2164 - accuracy: 0.8946 - val_loss: 0.1656 - val_accuracy: 0.9220\n",
      "Epoch 14/20\n",
      "5764/5764 [==============================] - 320s 55ms/step - loss: 0.2067 - accuracy: 0.8963 - val_loss: 0.1932 - val_accuracy: 0.8924\n",
      "Epoch 15/20\n",
      "5764/5764 [==============================] - 320s 56ms/step - loss: 0.2058 - accuracy: 0.9040 - val_loss: 0.2581 - val_accuracy: 0.8742\n",
      "Epoch 16/20\n",
      "5764/5764 [==============================] - 315s 55ms/step - loss: 0.2139 - accuracy: 0.8920 - val_loss: 0.1584 - val_accuracy: 0.9220\n",
      "Epoch 17/20\n",
      "5764/5764 [==============================] - 311s 54ms/step - loss: 0.2138 - accuracy: 0.8912 - val_loss: 0.3289 - val_accuracy: 0.8662\n",
      "Epoch 18/20\n",
      "5764/5764 [==============================] - 312s 54ms/step - loss: 0.2322 - accuracy: 0.8847 - val_loss: 0.1870 - val_accuracy: 0.8874\n",
      "Epoch 19/20\n",
      "5764/5764 [==============================] - 322s 56ms/step - loss: 0.2297 - accuracy: 0.8857 - val_loss: 0.2085 - val_accuracy: 0.8861\n",
      "Epoch 20/20\n",
      "5764/5764 [==============================] - 303s 53ms/step - loss: 0.2504 - accuracy: 0.8793 - val_loss: 0.2056 - val_accuracy: 0.8874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f984a7fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_GRU2.fit(x_reduced_train, y_reduced_train, batch_size=64, \n",
    "          epochs=20, validation_data=(x_reduced_test, y_reduced_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a469a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8873971087420968 0.7596624997012215 0.9897851136717534 0.859588624284961\n"
     ]
    }
   ],
   "source": [
    "prediction_gru2 = model_GRU2.predict(x_reduced_test)\n",
    "prediction_gru2 = prediction_gru2.flatten()\n",
    "prediction_gru2 = [int(round(i,0)) for i in prediction_gru2]\n",
    "\n",
    "accuracy = accuracy_score(y_reduced_test, prediction_gru2)\n",
    "precision = precision_score(y_reduced_test, prediction_gru2, average='binary')\n",
    "recall = recall_score(y_reduced_test, prediction_gru2, average='binary')\n",
    "f1_ = f1_score(y_reduced_test, prediction_gru2, average='binary')\n",
    "\n",
    "print(accuracy, precision, recall, f1_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236ec4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
